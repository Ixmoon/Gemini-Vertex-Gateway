# file: getkeys.py

import sys
import os
import random
import tempfile
import shutil
import time
import concurrent.futures
import queue
import threading
import multiprocessing
import logging
import logging.handlers
from functools import wraps
from google.api_core import exceptions as google_exceptions
from retry_utils import robust_retry
from google.cloud import resourcemanager, service_usage, api_keys
from google.oauth2 import credentials as google_credentials
from auto_login import perform_login_automation
import undetected_chromedriver as uc
from constants import (STATUS_PROCESSING, STATUS_SUCCESS, STATUS_FAILURE,
                        STATUS_PARTIAL_SUCCESS, STATUS_PENDING,
                        GCloudNotInstalledError)

# --- 并发控制常量 ---
PROJECT_CHECK_CONCURRENCY = 12
PROJECT_CREATE_CONCURRENCY = 6
PROJECT_CONFIG_CONCURRENCY = 12

# --- 线程/进程局部存储 ---
thread_local = threading.local()

# --- 日志配置 ---
class SafeFormatter(logging.Formatter):
    """
    A formatter that defensively adds context fields to any log record,
    preventing KeyErrors from records generated by external libraries.
    """
    def format(self, record):
        record.account = getattr(record, 'account', getattr(thread_local, 'account', 'N/A'))
        record.project = getattr(record, 'project', getattr(thread_local, 'project', '-----'))
        return super().format(record)

def setup_global_logging(log_queue=None, level=logging.INFO):
    root_logger = logging.getLogger()
    if root_logger.hasHandlers():
        root_logger.handlers.clear()
    
    root_logger.setLevel(level)
    # The ContextFilter is removed in favor of the SafeFormatter.
    
    formatter = SafeFormatter(
        '[%(asctime)s] [%(levelname)-5s] [%(account)s] [%(project)s] %(message)s',
        datefmt='%H:%M:%S'
    )
    
    handler = logging.handlers.QueueHandler(log_queue) if log_queue else logging.StreamHandler(sys.stdout)
    handler.setFormatter(formatter)
    root_logger.addHandler(handler)

# --- 核心业务逻辑函数 ---
def check_gcloud_installed():
    gcloud_path = shutil.which('gcloud')
    if not gcloud_path:
        raise GCloudNotInstalledError("未找到 'gcloud' 命令行工具。")
    return gcloud_path

@robust_retry(max_retries=3, delay=3, backoff=2) # Tuned for ~30s total timeout
def enable_api_if_not_enabled(su_client, project_id, logger, stop_event):
    if stop_event.is_set(): raise InterruptedError("Operation cancelled.")
    service_name = f"projects/{project_id}/services/generativelanguage.googleapis.com"
    try:
        service = su_client.get_service(request={'name': service_name})
        if service.state == service_usage.State.ENABLED:
            logger.info("✅ API 已启用。")
            return True
    except google_exceptions.NotFound:
        logger.info("ℹ️ API 未启用，正在尝试启用...")
    
    operation = su_client.enable_service(request={'name': service_name})
    operation.result(timeout=300)
    logger.info("✅ API 启用成功。")
    return True

@robust_retry(max_retries=3, delay=3, backoff=2) # Tuned for ~30s total timeout
def create_new_key(ak_client, project_id, logger, stop_event):
    """Creates a new key without checking for existing ones."""
    if stop_event.is_set(): raise InterruptedError("Operation cancelled.")
    
    logger.info("ℹ️ 正在创建新的API密钥...")
    key_request = api_keys.CreateKeyRequest(
        parent=f"projects/{project_id}/locations/global",
        key=api_keys.Key(display_name="Auto-Generated Gemini Key")
    )
    operation = ak_client.create_key(request=key_request)
    api_key_obj = operation.result(timeout=300)
    logger.info("✅ API密钥创建成功。")
    return ak_client.get_key_string(request={'name': api_key_obj.name}).key_string

@robust_retry(max_retries=3, delay=5) # Read operations can be retried faster
def get_existing_key(ak_client, project_id, logger, stop_event):
    """A read-only function to find an existing, unrestricted API key."""
    if stop_event.is_set(): raise InterruptedError("Operation cancelled.")
    
    keys = ak_client.list_keys(parent=f"projects/{project_id}/locations/global")
    for key in keys:
        if key.restrictions is None or not key.restrictions.api_targets:
            # Found a usable key, get its string and return.
            return ak_client.get_key_string(request={'name': key.name}).key_string
    return None # No usable key found

def _execute_with_context(func, account_email, project_id, *args):
    """A wrapper to set thread-local context before executing a function."""
    try:
        thread_local.account = account_email
        thread_local.project = project_id
        return func(*args)
    finally:
        thread_local.project = '-----'

def process_existing_project(project_id, account_email, logger, stop_event, su_client, ak_client):
    """Processes a single project to enable API and get a key in parallel."""
    if stop_event.is_set(): return None
    
    key_string = None
    api_enabled = False
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        # Simultaneously execute API enabling and key creation
        future_enable_api = executor.submit(_execute_with_context, enable_api_if_not_enabled, account_email, project_id, su_client, project_id, logger, stop_event)
        future_create_key = executor.submit(_execute_with_context, create_new_key, account_email, project_id, ak_client, project_id, logger, stop_event)
        
        # Gather results independently to allow one to succeed even if the other fails.
        try:
            api_enabled = future_enable_api.result()
        except Exception as e:
            if not isinstance(e, InterruptedError):
                logger.error(f"❌ 启用API时出错: {e}", exc_info=True)
            api_enabled = False # Explicitly mark as failed
        
        try:
            key_string = future_create_key.result()
        except Exception as e:
            if not isinstance(e, InterruptedError):
                logger.error(f"❌ 创建密钥时出错: {e}", exc_info=True)
            key_string = None # Explicitly mark as failed

    # Only return the key if both operations were successful
    if api_enabled and key_string:
        return key_string
    else:
        # The specific errors have already been logged. This provides a summary.
        logger.error(f"❌ 未能完成项目 {project_id} 的所有配置步骤 (API启用: {api_enabled}, 密钥获取: {key_string is not None})")
        return None

def handle_existing_projects(account_email, logger, all_keys, lock, projects, desired_keys, stop_event, su_client, ak_client, failure_count):
    if not projects: return
    thread_local.account = account_email
    logger.info(f"--- (任务1) 开始'读写分离'模式处理 {len(projects)} 个现有项目 ---")

    projects_needing_work = []
    
    # --- Phase 1: Read-Only Discovery ---
    logger.info("--- (阶段1) 并发查询所有项目的现有密钥... ---")
    with concurrent.futures.ThreadPoolExecutor(max_workers=PROJECT_CHECK_CONCURRENCY) as executor:
        future_to_project = {
            executor.submit(_execute_with_context, get_existing_key, account_email, p.project_id, ak_client, p.project_id, logger, stop_event): p
            for p in projects
        }

        for future in concurrent.futures.as_completed(future_to_project):
            project = future_to_project[future]
            try:
                key_string = future.result()
                if key_string:
                    # Read phase successful: Found a key.
                    with lock:
                        if key_string not in all_keys:
                            all_keys.add(key_string)
                            logger.info(f"项目 {project.project_id} 发现已有密钥。")
                            if desired_keys > 0 and len(all_keys) >= desired_keys:
                                stop_event.set()
                else:
                    # Read phase successful: Confirmed no key exists. Add to write queue.
                    projects_needing_work.append(project)
            except (concurrent.futures.CancelledError, InterruptedError):
                # If cancelled, do nothing. The stop_event will handle termination.
                pass
            except Exception:
                # Read phase failed after all retries. The state is unknown.
                # Mark as failure and DO NOT proceed to write phase for this project.
                with lock:
                    failure_count[0] += 1
                logger.error(f"❌ 在阶段1无法确定项目 {project.project_id} 的密钥状态，已跳过。")

    if stop_event.is_set():
        logger.info("--- (任务1) 因达到目标或用户中断，提前结束 ---")
        return

    # --- Phase 2: Write Operations ---
    if not projects_needing_work:
        logger.info("--- (阶段2) 无需处理任何项目，所有项目均已找到可用密钥。 ---")
        logger.info("--- (任务1) 处理现有项目完成 ---")
        return

    logger.info(f"--- (阶段2) 对 {len(projects_needing_work)} 个项目进行API启用和密钥创建... ---")
    with concurrent.futures.ThreadPoolExecutor(max_workers=PROJECT_CHECK_CONCURRENCY) as executor:
        future_to_project_id = {
            executor.submit(process_existing_project, p.project_id, account_email, logger, stop_event, su_client, ak_client): p.project_id
            for p in projects_needing_work
        }

        for future in concurrent.futures.as_completed(future_to_project_id):
            if stop_event.is_set(): break
            project_id = future_to_project_id[future]
            try:
                key = future.result()
                if key:
                    with lock:
                        all_keys.add(key)
                        if desired_keys > 0 and len(all_keys) >= desired_keys:
                            stop_event.set()
                else:
                    with lock:
                        failure_count[0] += 1
            except (concurrent.futures.CancelledError, InterruptedError):
                pass
            except Exception as e:
                logger.error(f"❌ 在项目 {project_id} 中获取密钥时发生意外错误: {e}", exc_info=True)
                with lock:
                    failure_count[0] += 1
    
    logger.info("--- (任务1) 处理现有项目完成 ---")

def creator_task(account_email, projects_to_configure_queue, logger, existing_project_names, projects_to_create, stop_event, rm_client):
    """
    Concurrently creates new projects and puts their IDs into a queue for configuration.
    """
    def _create_and_queue_project(project_id, quota_hit_event):
        """Worker function to create a single project."""
        if stop_event.is_set() or quota_hit_event.is_set():
            return
        try:
            _execute_with_context(logger.info, account_email, project_id, f"▶️ 提交创建新项目 '{project_id}'...")
            operation = rm_client.create_project(project=resourcemanager.Project(project_id=project_id, display_name=project_id))
            operation.result(timeout=300)  # This is a blocking call.

            if stop_event.is_set(): # Check again after the long wait for a GLOBAL stop.
                return

            _execute_with_context(logger.info, account_email, project_id, f"✅ 项目 '{project_id}' 创建成功，已加入配置队列。")
            projects_to_configure_queue.put(project_id)
        except (google_exceptions.ResourceExhausted, google_exceptions.PermissionDenied) as e:
            logger.warning(f"ℹ️ 已达到项目创建配额，将停止创建新项目。")
            # Set the event to signal the main loop to stop submitting new tasks.
            quota_hit_event.set()
        except Exception as e:
            if not stop_event.is_set():
                logger.error(f"❌ 创建项目 '{project_id}' 时出错: {e}", exc_info=True)

    if projects_to_create <= 0:
        logger.info("创建者任务结束。")
        return

    project_names_to_create = []
    # This name generation loop is single-threaded, so no lock is needed for existing_project_names.
    # Pre-generate a list of unique project IDs based on a timestamp and an incrementing counter.
    base_name = f"my-project-{int(time.time())}"
    i = 0
    while len(project_names_to_create) < projects_to_create:
        if stop_event.is_set():
            break
        display_name = f"{base_name}-{i}"
        if display_name not in existing_project_names:
            # Only add the name if it's not already taken.
            project_names_to_create.append(display_name)
        i += 1
    
    # Reserve all the generated names at once.
    existing_project_names.update(project_names_to_create)

    if not project_names_to_create:
        logger.info("创建者任务结束 (没有项目需要创建)。")
        return

    logger.info(f"计划并发创建 {len(project_names_to_create)} 个项目 (并发度: {PROJECT_CREATE_CONCURRENCY})...")

    quota_hit_event = threading.Event()
    with concurrent.futures.ThreadPoolExecutor(max_workers=PROJECT_CREATE_CONCURRENCY) as executor:
        futures = []
        for name in project_names_to_create:
            if stop_event.is_set() or quota_hit_event.is_set():
                logger.info("创建任务因外部信号或达到配额而提前中止。")
                break
            # Pass the quota_hit_event to the worker
            futures.append(executor.submit(_create_and_queue_project, name, quota_hit_event))
        
        if futures:
            # Use as_completed to process futures as they finish. This allows the
            # loop to check the stop_event frequently and exit early, rather than
            # waiting for all futures to complete, which is crucial if one
            # of them signals a stop.
            for future in concurrent.futures.as_completed(futures):
                if stop_event.is_set():
                    # A stop was signaled (likely by a task that just completed).
                    # We can now break the loop and allow the creator_task to exit,
                    # without waiting for other long-running API calls.
                    # We can also attempt to cancel any remaining futures.
                    # Note: cancel() won't interrupt a running operation.result(),
                    # but it will prevent queued tasks from starting.
                    for f in futures:
                        if not f.done():
                            f.cancel()
                    break

    logger.info("创建者任务结束。")

def configurator_worker(projects_to_configure_queue, account_email, logger, all_keys, lock, desired_keys, stop_event, su_client, ak_client, failure_count):
    while not stop_event.is_set():
        try:
            project_id = projects_to_configure_queue.get(timeout=1)
            if project_id is None: # "Poison pill" received
                break
            key = process_existing_project(project_id, account_email, logger, stop_event, su_client, ak_client)
            if key:
                with lock:
                    all_keys.add(key)
                    if desired_keys > 0 and len(all_keys) >= desired_keys:
                        stop_event.set() # Signal to stop all other tasks
            else:
                with lock:
                    failure_count[0] += 1
            projects_to_configure_queue.task_done()
        except queue.Empty:
            continue

def handle_new_projects(account_email, logger, all_keys, lock, projects, projects_to_create, stop_event, desired_keys, rm_client, su_client, ak_client, failure_count):
    thread_local.account = account_email
    logger.info("--- (任务2) 启动创建者/配置者模式 ---")
    projects_to_configure_queue = queue.Queue()
    existing_project_names = {p.display_name for p in projects}

    creator = threading.Thread(target=creator_task, args=(account_email, projects_to_configure_queue, logger, existing_project_names, projects_to_create, stop_event, rm_client))
    creator.start()

    with concurrent.futures.ThreadPoolExecutor(max_workers=PROJECT_CONFIG_CONCURRENCY) as executor:
        config_futures = [executor.submit(configurator_worker, projects_to_configure_queue, account_email, logger, all_keys, lock, desired_keys, stop_event, su_client, ak_client, failure_count) for _ in range(PROJECT_CONFIG_CONCURRENCY)]
        
        creator.join()
        
        # Gracefully shutdown configurator workers
        for _ in range(PROJECT_CONFIG_CONCURRENCY):
            projects_to_configure_queue.put(None)

        for f in config_futures:
            try:
                if stop_event.is_set():
                    f.cancel()
                f.result()
            except (concurrent.futures.CancelledError, InterruptedError):
                pass
    
    logger.info("--- (任务2) 处理新项目完成 ---")

def _run_account_processing_logic(account_email, password, configs, worker_id, stop_event, gui_queue, gcloud_path, patched_driver_path, result, lock, failure_count, temp_dir, logger):
    """Inner function containing the core processing logic for an account."""
    desired_keys = int(configs.get("desired_keys", 0))
    try:
        if stop_event.is_set(): raise InterruptedError("任务在启动前被取消。")
        
        # 直接向GUI队列发送状态更新
        gui_queue.put({"account": account_email, "status": STATUS_PROCESSING, "pid": os.getpid()})
        logger.info(f"--- 开始处理账号 (进程PID: {os.getpid()}) ---")

        @robust_retry(max_retries=2, delay=3)
        def attempt_login(stop_event_proxy):
            if stop_event_proxy.is_set(): raise InterruptedError("Login cancelled.")
            success = perform_login_automation(
                account_email, password, gcloud_path, temp_dir, patched_driver_path,
                stop_event_proxy, logger, configs.get("browser_path"), worker_id, configs.get("window_size"), configs.get("window_position")
            )
            if not success:
                if stop_event_proxy.is_set(): raise InterruptedError("Login failed due to stop signal.")
                raise RuntimeError("登录失败")
            return success
        
        attempt_login(stop_event)
        logger.info("✅ 登录成功。")

        adc_path = os.path.join(temp_dir, "application_default_credentials.json")
        credentials = google_credentials.Credentials.from_authorized_user_file(adc_path)
        
        rm_client = resourcemanager.ProjectsClient(credentials=credentials)
        su_client = service_usage.ServiceUsageClient(credentials=credentials)
        ak_client = api_keys.ApiKeysClient(credentials=credentials)

        def _run_api_tasks():
            """Encapsulates the post-login API-heavy tasks."""
            all_projects = list(rm_client.search_projects())
            logger.info(f"✅ 发现 {len(all_projects)} 个项目。")

            projects_to_create = 0
            if desired_keys == 0:
                projects_to_create = 100
            else:
                if desired_keys > len(result["keys"]):
                     projects_to_create = desired_keys - len(result["keys"])

            t1 = threading.Thread(target=handle_existing_projects, args=(account_email, logger, result["keys"], lock, all_projects, desired_keys, stop_event, su_client, ak_client, failure_count))
            t2 = threading.Thread(target=handle_new_projects, args=(account_email, logger, result["keys"], lock, all_projects, projects_to_create, stop_event, desired_keys, rm_client, su_client, ak_client, failure_count))
            
            t1.start()
            t2.start()
            t1.join()
            t2.join()

        # --- Timeout logic now only wraps the API tasks ---
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            api_future = executor.submit(_run_api_tasks)
            try:
                api_future.result(timeout=30)
            except concurrent.futures.TimeoutError:
                # DO NOT set the global stop_event. A local timeout should not stop other processes.
                # The raise InterruptedError is sufficient to terminate this account's processing.
                logger.warning("--- API处理超过30秒超时，正在中止... ---")
                if result["keys"]:
                    result["status"] = STATUS_PARTIAL_SUCCESS
                    result["reason"] = "API处理超时"
                else:
                    result["status"] = STATUS_FAILURE
                    result["reason"] = "API处理超时，未获得任何密钥"
                # If we timeout, we must exit the main try block to avoid re-evaluating status.
                raise InterruptedError("API处理超时")
        
        # --- Final status evaluation ---
        final_failure_count = failure_count[0]
        goal_was_achieved = stop_event.is_set() and (desired_keys > 0 and len(result["keys"]) >= desired_keys)

        if result["keys"]:
            if goal_was_achieved:
                result["status"] = STATUS_SUCCESS
                result["reason"] = f"已达到目标密钥数 ({len(result['keys'])}个)"
                result['goal_achieved'] = True
            elif stop_event.is_set():
                result["status"] = STATUS_PARTIAL_SUCCESS
                result["reason"] = f"用户中断, {final_failure_count}个项目处理失败"
            elif final_failure_count > 0:
                result["status"] = STATUS_PARTIAL_SUCCESS
                result["reason"] = f"有 {final_failure_count} 个项目处理失败"
            else:
                result["status"] = STATUS_SUCCESS
        elif stop_event.is_set():
            result["status"] = STATUS_PENDING
            result["reason"] = "被用户强制中断"
        elif final_failure_count > 0:
            result["status"] = STATUS_FAILURE
            result["reason"] = f"所有尝试的项目 ({final_failure_count}个) 均处理失败"

    except InterruptedError as e:
        # This now catches the timeout error as well
        if "超时" not in result["reason"]: # Avoid overwriting the specific timeout message
            logger.warning(f"任务被用户强制中断: {e}")
            result['status'] = STATUS_PENDING
            result['reason'] = "被用户强制中断"
    except Exception as e:
        logger.critical(f"处理账号时发生顶层错误: {e}", exc_info=True)
        result['status'] = STATUS_FAILURE
        result['reason'] = str(e)

def process_account(account_email, password, configs, worker_id, stop_event, gui_queue, gcloud_path, patched_driver_path):
    # Logging now points directly to the gui_queue
    setup_global_logging(log_queue=gui_queue)
    logger = logging.getLogger()
    thread_local.account = account_email

    result = {"account": account_email, "keys": set(), "status": STATUS_FAILURE, "reason": ""}
    temp_dir = tempfile.mkdtemp()
    
    lock = threading.Lock()
    failure_count = [0]
    
    try:
        # The timeout logic is now inside _run_account_processing_logic, so we just call it directly.
        _run_account_processing_logic(
            account_email, password, configs, worker_id, stop_event, gui_queue,
            gcloud_path, patched_driver_path, result, lock, failure_count, temp_dir, logger
        )
    except Exception as e:
        # Catch any unexpected error during the process
        logger.critical(f"Executor in process_account failed: {e}", exc_info=True)
        result['status'] = STATUS_FAILURE
        result['reason'] = str(e)
    finally:
        shutil.rmtree(temp_dir, ignore_errors=True)
        result['keys'] = list(result['keys'])
    
    return result

def process_account_wrapper(args):
    account_email, password, configs, worker_id, stop_event_proxy, gui_queue, gcloud_path, patched_driver_path = args
    
    class StopEvent:
        def is_set(self):
            try: return stop_event_proxy.value == 1
            except Exception: return True
        
        def set(self):
            try:
                stop_event_proxy.value = 1
            except Exception:
                pass # Proxy might be gone, ignore
    
    return process_account(account_email, password, configs, worker_id, StopEvent(), gui_queue, gcloud_path, patched_driver_path)

def start_processing(executor, accounts, gui_queue, stop_event, configs):
    # --- Pre-computation and Initialization ---
    # Perform these checks once before starting worker processes to reduce redundant work.
    try:
        gcloud_path = check_gcloud_installed()
        browser_path = configs.get("browser_path")
        patcher = uc.Patcher(executable_path=browser_path) if browser_path and os.path.exists(browser_path) else uc.Patcher()
        patched_driver_path = patcher.executable_path if os.path.exists(patcher.executable_path) else patcher.auto()
    except Exception as e:
        # If initialization fails, report back to the GUI and stop.
        gui_queue.put({"account": "general_error", "status": STATUS_FAILURE, "reason": str(e)})
        return

    manager = multiprocessing.Manager()
    # log_queue and the listener thread are removed for simplification.
    stop_event_proxy = manager.Value('i', 0)

    def stop_listener():
        stop_event.wait()
        stop_event_proxy.value = 1
    threading.Thread(target=stop_listener, daemon=True).start()

    # Pass gui_queue directly to the workers.
    tasks_args = [(email, password, configs, i, stop_event_proxy, gui_queue, gcloud_path, patched_driver_path) for i, (email, password) in enumerate(accounts)]

    try:
        future_to_account = {executor.submit(process_account_wrapper, arg): arg[0] for arg in tasks_args}
        
        for future in concurrent.futures.as_completed(future_to_account):
            if stop_event.is_set():
                # 一旦检测到停止信号，立即中断循环，不再处理任何已完成的任务
                break
            try:
                result = future.result()
                gui_queue.put(result)
            except Exception as e:
                account = future_to_account[future]
                gui_queue.put({"account": account, "status": STATUS_FAILURE, "reason": f"进程崩溃: {e}"})

    except Exception as e:
        if not stop_event.is_set():
            logging.critical(f"--- 脚本主线程因意外而中断: {e} ---", exc_info=True)
    finally:
        # No listener thread to join anymore.
        logging.info("\n--- 所有任务已提交或被中断，主进程退出 ---")
